{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Unpack and load the [AMASS][] dataset for training with a PyTorch iterator.\n",
    "\n",
    "\n",
    "\n",
    "[amass]: https://amass.is.tue.mpg.de/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpack Tar Files\n",
    "\n",
    "> Console script to unpack all tar files found in a specified directory and put them in another directory, then create a symlink to be able to find the unpacked data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import argparse\n",
    "import os\n",
    "from shutil import unpack_archive\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class ProgressParallel(joblib.Parallel):\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(total=kwargs['total']) as self._pbar:\n",
    "            del kwargs['total']\n",
    "            return joblib.Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        self._pbar.total = self.n_dispatched_tasks\n",
    "        self._pbar.n = self.n_completed_tasks\n",
    "        self._pbar.refresh()\n",
    "\n",
    "def unpack_body_models(tardir, outdir, n_jobs=1):\n",
    "    tar_root, _, tarfiles = [x for x in os.walk(tardir)][0]\n",
    "    tarfiles = [x for x in tarfiles if 'tar' in x.split('.')]\n",
    "    tarpaths = [os.path.join(tar_root, tar) for tar in tarfiles]\n",
    "    for tarpath in tarpaths:\n",
    "        print(f\"{tarpath} extracting to {outdir}\")\n",
    "    ProgressParallel(n_jobs=n_jobs)((joblib.delayed(unpack_archive)(tarpath, outdir) for tarpath in tarpaths),\n",
    "                                     total=len(tarpaths))\n",
    "\n",
    "def fast_amass_unpack():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Unpack all the body model tar files in a directory to a target directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"tardir\",\n",
    "        type=str,\n",
    "        help=\"Directory containing tar.bz2 body model files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"outdir\",\n",
    "        type=str,\n",
    "        help=\"Output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of jobs to run the tar unpacking with\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    unpack_body_models(args.tardir, args.outdir, n_jobs=args.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unpacking the sample data always yields the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp7lorodj_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c694fc8436a445fbb9e18aff64c4ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "# https://stackoverflow.com/a/3431838/6937913\n",
    "def md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "md5sums = {'amass_sample.npz': 'd0b546b3619c8579ade39e3a8ccdc4e2',\n",
    "           'dmpl_sample.npz':  '576bb76b2a6328dc5c276c4150c466f0'}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if 'npz' in x.split('.')]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    _md5sums = {os.path.split(fpath)[-1]:md5(fpath) for fpath in npz_paths}\n",
    "\n",
    "for k in md5sums:\n",
    "    assert md5sums[k] == _md5sums[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "> Load the pose data directly from the `npz` files after unpacking.\n",
    "\n",
    "Based on the [AMASS tutorial notebooks][amass], I would like to iterate over the dataset using a PyTorch Dataloader.\n",
    "\n",
    "Steps to load:\n",
    "\n",
    "1. Enumerate the paths to all the `npz` files\n",
    "2. Inspect files for frame data\n",
    "3. Map from a global dataset index to indexes for each frame clip\n",
    "\n",
    "[amass]: https://github.com/nghorbani/amass/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp7vlyrvuq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a27b4c28df40eb914ace0dd2c19780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp7vlyrvuq/sample/subdir/amass_sample.npz\n",
      "   ['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "   [('poses', (601, 156)), ('gender', ()), ('mocap_framerate', ()), ('betas', (16,)), ('marker_data', (601, 85, 3)), ('dmpls', (601, 8)), ('marker_labels', (85,)), ('trans', (601, 3))]\n",
      "/tmp/tmp7vlyrvuq/sample/subdir/dmpl_sample.npz\n",
      "   ['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "   [('poses', (235, 156)), ('gender', ()), ('mocap_framerate', ()), ('betas', (16,)), ('marker_data', (235, 67, 3)), ('dmpls', (235, 8)), ('marker_labels', (67,)), ('trans', (235, 3))]\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if 'npz' in x.split('.')]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    for npz_path in npz_paths:\n",
    "        cdata = np.load(npz_path)\n",
    "        print(npz_path)\n",
    "        print(\"  \", [k for k in cdata.keys()])\n",
    "        print(\"  \", [(k, cdata[k].shape) for k in cdata.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viable Indexes\n",
    "\n",
    "For every `npz` file I need to pull out the viable indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def viable_slice(cdata, keep):\n",
    "    \"\"\"\n",
    "    Inspects a dictionary loaded from `.npz` numpy dumps \n",
    "    and creates a slice of the viable indexes.\n",
    "    args:\n",
    "    \n",
    "        - `cdata`: dictionary containing keys:\n",
    "            ['poses', 'gender', 'mocap_framerate', 'betas',\n",
    "             'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
    "        - `keep`: ratio of the file to keep, between zero and 1.,\n",
    "            drops leading and trailing ends of the arrays\n",
    "\n",
    "    returns:\n",
    "\n",
    "        - viable: slice that can access frames in the arrays:\n",
    "            cdata['poses'], cdata['marker_data'], cdata['dmpls'], cdata['trans']\n",
    "    \"\"\"\n",
    "    assert keep > 0. and keep <=1.0, \"Proportion of array to keep must be between zero and one\"\n",
    "    n = cdata['poses'].shape[0]\n",
    "    drop = (1.-keep)/2.\n",
    "    return slice(int(n*drop), int(n*keep+n*drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmppksocojp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc32ba210b034befa7aeb8f3a565695d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmppksocojp/sample/subdir/amass_sample.npz\n",
      "   slice(60, 540, None)\n",
      "/tmp/tmppksocojp/sample/subdir/dmpl_sample.npz\n",
      "   slice(23, 211, None)\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if 'npz' in x.split('.')]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    for npz_path in npz_paths:\n",
    "        cdata = np.load(npz_path)\n",
    "        print(npz_path)\n",
    "        print(\"  \", viable_slice(cdata, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Global Index to File Indexes\n",
    "\n",
    "I need to be able to map from a global dataset index to contiguous sets of frames in each file. Options:\n",
    "\n",
    "1. Whether the contiguous frames can overlap in different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def global_index_map(npz_directory, overlapping, clip_length, keep=0.8):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        - `npz_directory`: Directory containing `.npz` files\n",
    "        - `overlapping`: Whether clips can overlap\n",
    "        - `clip_length`: \n",
    "    returns:\n",
    "        - map from global index to corresponding file and array indexes\n",
    "    \"\"\"\n",
    "    for r, d, f in os.walk(npz_directory):\n",
    "        npz_files = [x for x in f if 'npz' in x.split('.')]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    # array slices for each file\n",
    "    viable_slices = {npz_path:viable_slice(np.load(npz_path), keep=keep)\n",
    "                     for npz_path in npz_paths}\n",
    "    # clip index -> array index\n",
    "    def clip_to_array_index(i, array_slice):\n",
    "        if not overlapping:\n",
    "            i = i*clip_length\n",
    "        return [i + array_slice.start + j for j in range(clip_length)]\n",
    "    # length of a slice\n",
    "    def lenslice(s):\n",
    "        if overlapping:\n",
    "            return (s.stop - s.start) - (clip_length-1)\n",
    "        else:\n",
    "            return math.floor((s.stop - s.start)/clip_length)\n",
    "    # global index -> file, relative index\n",
    "    def find_array(i):\n",
    "        global_index, j = 0, 0\n",
    "        for npz_path in viable_slices:\n",
    "            global_index += lenslice(viable_slices[npz_path])\n",
    "            if i < global_index:\n",
    "                return npz_path, i - j\n",
    "            j = global_index\n",
    "    # how many examples are there in this dataset, total\n",
    "    n_examples = sum(lenslice(viable_slices[npz_path])\n",
    "                     for npz_path in viable_slices)\n",
    "    # create map function\n",
    "    def global_to_array(i):\n",
    "        npz_path, j = find_array(i)\n",
    "        return npz_path, clip_to_array_index(j, viable_slices[npz_path])\n",
    "    return global_to_array, n_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for clip length 1 and non-overlapping clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmptypp4_vk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1841db3324b4dc79444e3b8460b9857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 668\n"
     ]
    }
   ],
   "source": [
    "def test_global_index_map(clip_length, overlapping):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "        global_to_array, n_examples = global_index_map(tmpdirname, overlapping=overlapping, clip_length=clip_length)\n",
    "        print(f\"Number of examples in dataset: {n_examples}\")\n",
    "        count = 0\n",
    "        prev_j = [-1]*clip_length\n",
    "        for i in range(n_examples):\n",
    "            npz_path, j = global_to_array(i)\n",
    "            cdata = np.load(npz_path)\n",
    "            assert len(j) == clip_length\n",
    "            assert cdata['poses'][j] is not None\n",
    "            assert all(k > 0 for k in j)\n",
    "            if not overlapping:\n",
    "                assert all(k not in prev_j for k in j)\n",
    "                prev_j = j\n",
    "            count += 1\n",
    "        assert count == n_examples\n",
    "\n",
    "test_global_index_map(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for clip length greater than 1 and non-overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp5zz2ebv8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dcc4e65add434d90a113237d991d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 222\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpun35vwng\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0449d43daf4fa187b3c65662b6c56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 167\n"
     ]
    }
   ],
   "source": [
    "test_global_index_map(3, False)\n",
    "test_global_index_map(4, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for length of clip less than 1 and overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpz5k5_dpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c40152c0a3437a87f9597e4c7023b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 664\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp0cwojfr2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1186f81098441848dc24a67fa3a3e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 662\n"
     ]
    }
   ],
   "source": [
    "test_global_index_map(3, True)\n",
    "test_global_index_map(4, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from `.npz` Files\n",
    "\n",
    "I want to load the data from the `.npz` files in a standard way, so I'm going to load each entry into its own array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_npz(npz_path, indexes):\n",
    "    cdata = np.load(npz_path)\n",
    "    \n",
    "    # unpack and enforce data type\n",
    "    poses = cdata['poses'][indexes].astype(np.float32)\n",
    "    dmpls = cdata['dmpls'][indexes].astype(np.float32)\n",
    "    trans = cdata['trans'][indexes].astype(np.float32)\n",
    "    betas = np.repeat(cdata['betas'][np.newaxis].astype(np.float32), repeats=len(indexes), axis=0)\n",
    "    def gender_to_int(g):\n",
    "        # casting gender to integer will raise a warning in future\n",
    "        g = str(g.astype(str))\n",
    "        return {'male':-1, 'neutral':0, 'female':1}[g]\n",
    "    gender = np.array([gender_to_int(cdata['gender']) for _ in indexes])\n",
    "    \n",
    "    return dict(poses=poses, dmpls=dmpls, trans=trans, betas=betas, gender=gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this works with different clip lengths and overlapping clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpj48mc5xv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8594ed23d6474e5c91eb9cf6d3d07dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 156), (1, 8), (1, 3), (1, 16), (1,)]\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp_bxj2v43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d956da05d54193a1b0427b445266c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 156), (3, 8), (3, 3), (3, 16), (3,)]\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmphi85c16q\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa4d62efd594b4e9bbecffd5ee2fb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 156), (3, 8), (3, 3), (3, 16), (3,)]\n"
     ]
    }
   ],
   "source": [
    "def test_load_npz(clip_length, overlapping):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "        global_to_array, n_examples = global_index_map(tmpdirname, overlapping=overlapping, clip_length=clip_length)\n",
    "        npz_path, indexes = global_to_array(0)\n",
    "        data = load_npz(npz_path, indexes)\n",
    "        for k in data:\n",
    "            assert data[k].shape[0] == clip_length\n",
    "        print([data[k].shape for k in data])\n",
    "        \n",
    "test_load_npz(1, False)\n",
    "test_load_npz(3, False)\n",
    "test_load_npz(3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset Class\n",
    "\n",
    "Creating a map-style PyTorch Dataset Class that uses these functions to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AMASS(Dataset):\n",
    "    def __init__(self, unpacked_directory, clip_length, overlapping, transform=None):\n",
    "        self.global_to_array, self.n_examples = \\\n",
    "            global_index_map(unpacked_directory, overlapping=overlapping, clip_length=clip_length)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = load_npz(*self.global_to_array(i))\n",
    "        return {k:self.transform(data[k]) for k in data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test I can load some data with this Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpwn2q_8a2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895dca04a7f84a3ebfc0a41247bc6e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([1, 156])\n",
      "dmpls torch.Size([1, 8])\n",
      "trans torch.Size([1, 3])\n",
      "betas torch.Size([1, 16])\n",
      "gender torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    amass = AMASS(tmpdirname, overlapping=False, clip_length=1, transform=torch.tensor)\n",
    "    data = amass[0]\n",
    "    for k in data:\n",
    "        print(k, data[k].shape)\n",
    "        assert type(data[k]) is torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it works in a DataLoader to make batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp3j7vl7v3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d9c62b8e9a421086e069368a2da93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([4, 1, 156])\n",
      "dmpls torch.Size([4, 1, 8])\n",
      "trans torch.Size([4, 1, 3])\n",
      "betas torch.Size([4, 1, 16])\n",
      "gender torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    amass = AMASS(tmpdirname, overlapping=False, clip_length=1, transform=torch.tensor)\n",
    "    amasstrain = DataLoader(amass, batch_size=4, shuffle=True)\n",
    "    for i, data in enumerate(amasstrain):\n",
    "        if i == 0:\n",
    "            for k in data:\n",
    "                print(k, data[k].shape)\n",
    "        assert data['poses'].size(0) == 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
