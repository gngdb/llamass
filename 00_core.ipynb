{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Unpack and load the [AMASS][] dataset for training with a PyTorch iterator.\n",
    "\n",
    "To do:\n",
    "\n",
    "1. Saw `AMASS` and `global_index_map` break with relative paths, add test and make fix for this case\n",
    "2. Add options to pass `keep` index to `AMASS`\n",
    "\n",
    "[amass]: https://amass.is.tue.mpg.de/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpack Tar Files\n",
    "\n",
    "> Console script to unpack all tar files found in a specified directory and put them in another directory, then create a symlink to be able to find the unpacked data later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checksum Directories\n",
    "\n",
    "> Checksum directories to only unpack tar files when target directory either doesn't exist or has been incorrectly unpacked.\n",
    "\n",
    "It would probably be sufficient to check if the target directory exists, but this is more thorough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# https://stackoverflow.com/a/54477583/6937913\n",
    "import hashlib\n",
    "from _hashlib import HASH as Hash\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def md5_update_from_file(filename: Union[str, Path], hash: Hash) -> Hash:\n",
    "    assert Path(filename).is_file()\n",
    "    with open(str(filename), \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash.update(chunk)\n",
    "    return hash\n",
    "\n",
    "\n",
    "def md5_file(filename: Union[str, Path]) -> str:\n",
    "    return str(md5_update_from_file(filename, hashlib.md5()).hexdigest())\n",
    "\n",
    "\n",
    "def md5_update_from_dir(directory: Union[str, Path], hash: Hash) -> Hash:\n",
    "    assert Path(directory).is_dir()\n",
    "    for path in sorted(Path(directory).iterdir(), key=lambda p: str(p).lower()):\n",
    "        hash.update(path.name.encode())\n",
    "        if path.is_file():\n",
    "            hash = md5_update_from_file(path, hash)\n",
    "        elif path.is_dir():\n",
    "            hash = md5_update_from_dir(path, hash)\n",
    "    return hash\n",
    "\n",
    "\n",
    "def md5_dir(directory: Union[str, Path]) -> str:\n",
    "    return str(md5_update_from_dir(directory, hashlib.md5()).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "hashes = \\\n",
    "{'ACCAD.tar.bz2': {'unpacks_to': 'ACCAD',\n",
    "  'hash': '193442a2ab66cb116932b8bce08ecb89'},\n",
    " 'BMLhandball.tar.bz2': {'unpacks_to': 'BMLhandball',\n",
    "  'hash': '8947df17dd59d052ae618daf24ccace3'},\n",
    " 'BMLmovi.tar.bz2': {'unpacks_to': 'BMLmovi',\n",
    "  'hash': '6dfb134273f284152aa2d0838d7529d5'},\n",
    " 'CMU.tar.bz2': {'unpacks_to': 'CMU',\n",
    "  'hash': 'f04bc3f37f3eafebfb12ba0cf706ca72'},\n",
    " 'DFaust67.tar.bz2': {'unpacks_to': 'DFaust_67',\n",
    "  'hash': '7e5f11ed897da72c5159ef3c747383b8'},\n",
    " 'EKUT.tar.bz2': {'unpacks_to': 'EKUT',\n",
    "  'hash': '221ee4a27a03afd1808cbb11af067879'},\n",
    " 'HumanEva.tar.bz2': {'unpacks_to': 'HumanEva',\n",
    "  'hash': 'ca781438b08caafd8a42b91cce905a03'},\n",
    " 'KIT.tar.bz2': {'unpacks_to': 'KIT',\n",
    "  'hash': '3813500a3909f6ded1a1fffbd27ff35a'},\n",
    " 'MPIHDM05.tar.bz2': {'unpacks_to': 'MPI_HDM05',\n",
    "  'hash': 'f76da8deb9e583c65c618d57fbad1be4'},\n",
    " 'MPILimits.tar.bz2': {'unpacks_to': 'MPI_Limits',\n",
    "  'hash': '72398ec89ff8ac8550813686cdb07b00'},\n",
    " 'MPImosh.tar.bz2': {'unpacks_to': 'MPI_mosh',\n",
    "  'hash': 'a00019cac611816b7ac5b7e2035f3a8a'},\n",
    " 'SFU.tar.bz2': {'unpacks_to': 'SFU',\n",
    "  'hash': 'cb10b931509566c0a49d72456e0909e2'},\n",
    " 'SSMsynced.tar.bz2': {'unpacks_to': 'SSM_synced',\n",
    "  'hash': '7cc15af6bf95c34e481d58ed04587b58'},\n",
    " 'TCDhandMocap.tar.bz2': {'unpacks_to': 'TCD_handMocap',\n",
    "  'hash': 'c500aa07973bf33ac1587a521b7d66d3'},\n",
    " 'TotalCapture.tar.bz2': {'unpacks_to': 'TotalCapture',\n",
    "  'hash': 'b2c6833d3341816f4550799b460a1b27'},\n",
    " 'Transitionsmocap.tar.bz2': {'unpacks_to': 'Transitions_mocap',\n",
    "  'hash': '705e8020405357d9d65d17580a6e9b39'},\n",
    " 'EyesJapanDataset.tar.bz2': {'unpacks_to': 'Eyes_Japan_Dataset',\n",
    "  'hash': 'd19fc19771cfdbe8efe2422719e5f3f1'},\n",
    " 'BMLrub.tar.bz2': {'unpacks_to': 'BioMotionLab_NTroje',\n",
    "  'hash': '8b82ffa6c79d42a920f5dde1dcd087c3'},\n",
    " 'DanceDB.tar.bz2': {'unpacks_to': 'DanceDB',\n",
    "  'hash': '9ce35953c4234489036ecb1c26ae38bc'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Unpacking with Joblib\n",
    "\n",
    "> Unpacks tar files in multiple jobs to speed up unpacking the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "from shutil import unpack_archive\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class ProgressParallel(joblib.Parallel):\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(total=kwargs[\"total\"]) as self._pbar:\n",
    "            del kwargs[\"total\"]\n",
    "            return joblib.Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        self._pbar.n = self.n_completed_tasks\n",
    "        self._pbar.refresh()\n",
    "\n",
    "def lazy_unpack(tarpath, outdir):\n",
    "    # check if this has already been unpacked by looking for hash file\n",
    "    tarpath, outdir = Path(tarpath), Path(outdir)\n",
    "    unpacks_to = hashes[tarpath.name]['unpacks_to']\n",
    "    hashpath = outdir / Path(unpacks_to+'.hash')\n",
    "    # if the hash exists and it's correct then assume the directory is correctly unpacked\n",
    "    if hashpath.exists():\n",
    "        with open(hashpath) as f:\n",
    "            h = f.read() # read hash\n",
    "        if h == hashes[tarpath.name]['hash']:\n",
    "            return None\n",
    "    else:\n",
    "        # if there's no stored hash or it doesn't match, unpack the tar file\n",
    "        unpack_archive(tarpath, outdir)\n",
    "        # calculate the hash of the unpacked directory and check it's the same\n",
    "        h = md5_dir(outdir/unpacks_to)\n",
    "        _h = hashes[tarpath.name]['hash']\n",
    "        assert h == _h,\\\n",
    "            f'Directory {outdir/unpacks_to} hash {h} != {_h}'\n",
    "        # save the calculated hash\n",
    "        with open(hashpath, 'w') as f:\n",
    "            f.write(h)\n",
    "\n",
    "def unpack_body_models(tardir, outdir, n_jobs=1, verify=False):\n",
    "    tar_root, _, tarfiles = [x for x in os.walk(tardir)][0]\n",
    "    tarfiles = [x for x in tarfiles if \"tar\" in x.split(\".\")]\n",
    "    tarpaths = [os.path.join(tar_root, tar) for tar in tarfiles]\n",
    "    for tarpath in tarpaths:\n",
    "        print(f\"{tarpath} extracting to {outdir}\")\n",
    "    unpack = lazy_unpack if verify else unpack_archive\n",
    "    ProgressParallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(unpack)(tarpath, outdir) for tarpath in tarpaths),\n",
    "        total=len(tarpaths),\n",
    "    )\n",
    "\n",
    "\n",
    "def fast_amass_unpack():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Unpack all the body model tar files in a directory to a target directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"tardir\",\n",
    "        type=str,\n",
    "        help=\"Directory containing tar.bz2 body model files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"outdir\",\n",
    "        type=str,\n",
    "        help=\"Output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verify\",\n",
    "        type=\"store_true\",\n",
    "        help=\"Verify the output by calculating a checksum, \"\n",
    "        \"ensures that each tar file will only be unpacked once.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-n\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of jobs to run the tar unpacking with\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    unpack_body_models(args.tardir, args.outdir, n_jobs=args.n, verify=args.verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unpacking the sample data always yields the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmps93l76ly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4d939d604a41ee88d83368fd31e881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "# https://stackoverflow.com/a/3431838/6937913\n",
    "def md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "md5sums = {\n",
    "    \"amass_sample.npz\": \"d0b546b3619c8579ade39e3a8ccdc4e2\",\n",
    "    \"dmpl_sample.npz\": \"576bb76b2a6328dc5c276c4150c466f0\",\n",
    "}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    _md5sums = {os.path.split(fpath)[-1]: md5(fpath) for fpath in npz_paths}\n",
    "\n",
    "for k in md5sums:\n",
    "    assert md5sums[k] == _md5sums[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that `verify=True` works as expected. Can redefine `hashes` here for testing without breaking the exported library because this cell doesn't get exported by `nbdev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp5unr3c6d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ab5d85eeb04e938568731c9e0588ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp5unr3c6d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc188dd951c4eeebabf48cc9e0575ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "hashes = {'sample.tar.bz2': {'unpacks_to': 'sample', 'hash': 'b5a86fe22ed2799d79101a532eb0ff27'}}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    start = time.time()\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8, verify=True)\n",
    "    unpacking_time = time.time() - start\n",
    "    start = time.time()\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8, verify=True)\n",
    "    skip_time = time.time() - start\n",
    "    assert unpacking_time > skip_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "> Load the pose data directly from the `npz` files after unpacking.\n",
    "\n",
    "Based on the [AMASS tutorial notebooks][amass], I would like to iterate over the dataset using a PyTorch Dataloader.\n",
    "\n",
    "Steps to load:\n",
    "\n",
    "1. Enumerate the paths to all the `npz` files\n",
    "2. Inspect files for frame data\n",
    "3. Map from a global dataset index to indexes for each frame clip\n",
    "\n",
    "[amass]: https://github.com/nghorbani/amass/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpephft3on\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e66e05415c44a1bbaea5606474b206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpephft3on/sample/subdir/amass_sample.npz\n",
      "   ['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "   [('poses', (601, 156)), ('gender', ()), ('mocap_framerate', ()), ('betas', (16,)), ('marker_data', (601, 85, 3)), ('dmpls', (601, 8)), ('marker_labels', (85,)), ('trans', (601, 3))]\n",
      "/tmp/tmpephft3on/sample/subdir/dmpl_sample.npz\n",
      "   ['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "   [('poses', (235, 156)), ('gender', ()), ('mocap_framerate', ()), ('betas', (16,)), ('marker_data', (235, 67, 3)), ('dmpls', (235, 8)), ('marker_labels', (67,)), ('trans', (235, 3))]\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    for npz_path in npz_paths:\n",
    "        cdata = np.load(npz_path)\n",
    "        print(npz_path)\n",
    "        print(\"  \", [k for k in cdata.keys()])\n",
    "        print(\"  \", [(k, cdata[k].shape) for k in cdata.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viable Indexes\n",
    "\n",
    "For every `npz` file I need to pull out the viable indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def viable_slice(cdata, keep):\n",
    "    \"\"\"\n",
    "    Inspects a dictionary loaded from `.npz` numpy dumps\n",
    "    and creates a slice of the viable indexes.\n",
    "    args:\n",
    "\n",
    "        - `cdata`: dictionary containing keys:\n",
    "            ['poses', 'gender', 'mocap_framerate', 'betas',\n",
    "             'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
    "        - `keep`: ratio of the file to keep, between zero and 1.,\n",
    "            drops leading and trailing ends of the arrays\n",
    "\n",
    "    returns:\n",
    "\n",
    "        - viable: slice that can access frames in the arrays:\n",
    "            cdata['poses'], cdata['marker_data'], cdata['dmpls'], cdata['trans']\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        keep > 0.0 and keep <= 1.0\n",
    "    ), \"Proportion of array to keep must be between zero and one\"\n",
    "    n = cdata[\"poses\"].shape[0]\n",
    "    drop = (1.0 - keep) / 2.0\n",
    "    return slice(int(n * drop), int(n * keep + n * drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp1dmwr08g\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4eb850d82954e9ab111a466c0b6d7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp1dmwr08g/sample/subdir/amass_sample.npz\n",
      "   slice(60, 540, None)\n",
      "/tmp/tmp1dmwr08g/sample/subdir/dmpl_sample.npz\n",
      "   slice(23, 211, None)\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    for npz_path in npz_paths:\n",
    "        cdata = np.load(npz_path)\n",
    "        print(npz_path)\n",
    "        print(\"  \", viable_slice(cdata, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Global Index to File Indexes\n",
    "\n",
    "I need to be able to map from a global dataset index to contiguous sets of frames in each file. Options:\n",
    "\n",
    "1. Whether the contiguous frames can overlap in different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import warnings\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=2)\n",
    "def walk_npz_paths(npz_directory):\n",
    "    npz_paths = []\n",
    "    for r, d, f in os.walk(npz_directory):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths += [os.path.join(npz_directory, r, x) for x in npz_files]\n",
    "    return tuple(npz_paths)\n",
    "\n",
    "def read_viable_slices(npz_paths, keep_percent):\n",
    "    keep = keep_percent/100.\n",
    "    viable = {}\n",
    "    for npz_path in npz_paths:\n",
    "        try:\n",
    "            # filter out npz files that don't contain pose data\n",
    "            if Path(npz_path).name not in ['shape.npz']:\n",
    "                viable[npz_path] = viable_slice(np.load(npz_path), keep=keep)\n",
    "        except KeyError as err:\n",
    "            warnings.warn(f'Archive {npz_path} does not contain correctly formatted data')\n",
    "            # raise Exception(f'Error in archive {npz_path}') from err\n",
    "    return viable\n",
    "\n",
    "def global_index_map(npz_directory, overlapping, clip_length, keep=0.8, cache_map=True):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        - `npz_directory`: Directory containing `.npz` files\n",
    "        - `overlapping`: Whether clips can overlap\n",
    "        - `clip_length`:\n",
    "    returns:\n",
    "        - map from global index to corresponding file and array indexes\n",
    "    \"\"\"\n",
    "    npz_paths = walk_npz_paths(npz_directory)\n",
    "    # array slices for each file\n",
    "    if cache_map:\n",
    "        cache_map_dir = Path(npz_directory)/Path('viable_slices_memory')\n",
    "        memory = joblib.Memory(cache_map_dir, verbose=0)\n",
    "        viable_slices = memory.cache(read_viable_slices)(npz_paths, int(100*keep))\n",
    "    else:\n",
    "        viable_slices = read_viable_slices(npz_paths, int(100*keep))\n",
    "    # clip index -> array index\n",
    "    def clip_to_array_index(i, array_slice):\n",
    "        if not overlapping:\n",
    "            i = i * clip_length\n",
    "        return [i + array_slice.start + j for j in range(clip_length)]\n",
    "\n",
    "    # length of a slice\n",
    "    def lenslice(s):\n",
    "        if overlapping:\n",
    "            return (s.stop - s.start) - (clip_length - 1)\n",
    "        else:\n",
    "            return math.floor((s.stop - s.start) / clip_length)\n",
    "\n",
    "    # global index -> file, relative index\n",
    "    def find_array(i):\n",
    "        global_index, j = 0, 0\n",
    "        for npz_path in viable_slices:\n",
    "            global_index += lenslice(viable_slices[npz_path])\n",
    "            if i < global_index:\n",
    "                return npz_path, i - j\n",
    "            j = global_index\n",
    "\n",
    "    # how many examples are there in this dataset, total\n",
    "    n_examples = sum(lenslice(viable_slices[npz_path]) for npz_path in viable_slices)\n",
    "    # create map function\n",
    "    def global_to_array(i):\n",
    "        npz_path, j = find_array(i)\n",
    "        return npz_path, clip_to_array_index(j, viable_slices[npz_path])\n",
    "\n",
    "    return global_to_array, n_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for clip length 1 and non-overlapping clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmps9k3hq75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65187e03e72474f9d379a8e0246edf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 668\n"
     ]
    }
   ],
   "source": [
    "def test_global_index_map(clip_length, overlapping):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "        global_to_array, n_examples = global_index_map(\n",
    "            tmpdirname, overlapping=overlapping, clip_length=clip_length\n",
    "        )\n",
    "        print(f\"Number of examples in dataset: {n_examples}\")\n",
    "        count = 0\n",
    "        prev_j = [-1] * clip_length\n",
    "        for i in range(n_examples):\n",
    "            npz_path, j = global_to_array(i)\n",
    "            cdata = np.load(npz_path)\n",
    "            assert len(j) == clip_length\n",
    "            assert cdata[\"poses\"][j] is not None\n",
    "            assert all(k > 0 for k in j)\n",
    "            if not overlapping:\n",
    "                assert all(k not in prev_j for k in j)\n",
    "                prev_j = j\n",
    "            count += 1\n",
    "        assert count == n_examples\n",
    "\n",
    "\n",
    "test_global_index_map(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for clip length greater than 1 and non-overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp8hvalka_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81882939045c4c20ad084204e93d21d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 222\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp3jih9nxz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e20e9ab37ba476791edc252b0e604db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 167\n"
     ]
    }
   ],
   "source": [
    "test_global_index_map(3, False)\n",
    "test_global_index_map(4, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for length of clip less than 1 and overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpta70cr8w\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c03233032a4acf8f588926f6c7b93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 664\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp7rgo6dtq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e18aed40346405a8b4843ad9c416b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 662\n"
     ]
    }
   ],
   "source": [
    "test_global_index_map(3, True)\n",
    "test_global_index_map(4, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from `.npz` Files\n",
    "\n",
    "I want to load the data from the `.npz` files in a standard way, so I'm going to load each entry into its own array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_npz(npz_path, indexes, load_poses=True, load_dmpls=True,\n",
    "             load_trans=True, load_betas=True, load_gender=True):\n",
    "    # cache this because we will often be accessing the same file multiple times\n",
    "    cdata = functools.lru_cache(maxsize=128)(np.load)(npz_path)\n",
    "\n",
    "    data = {}\n",
    "    # unpack and enforce data type\n",
    "    if load_poses:\n",
    "        data['poses'] = cdata[\"poses\"][indexes].astype(np.float32)\n",
    "    if load_dmpls:\n",
    "        data['dmpls'] = cdata[\"dmpls\"][indexes].astype(np.float32)\n",
    "    if load_trans:\n",
    "        data['trans'] = cdata[\"trans\"][indexes].astype(np.float32)\n",
    "    if load_betas:\n",
    "        data['betas'] = np.repeat(\n",
    "            cdata[\"betas\"][np.newaxis].astype(np.float32), repeats=len(indexes), axis=0\n",
    "        )\n",
    "    if load_gender:\n",
    "        def gender_to_int(g):\n",
    "            # casting gender to integer will raise a warning in future\n",
    "            g = str(g.astype(str))\n",
    "            return {\"male\": -1, \"neutral\": 0, \"female\": 1}[g]\n",
    "        data['gender'] = np.array([gender_to_int(cdata[\"gender\"]) for _ in indexes])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this works with different clip lengths and overlapping clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp2bevwk7s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427c3f45acaf4977843edb77e318b2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 156), (1, 8), (1, 3), (1, 16), (1,)]\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpm2j2ugk9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad41cc464af42cc88b3a6c0104351b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 156), (3, 8), (3, 3), (3, 16), (3,)]\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp4lloez87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb85bf194dc34547aeb9f033fbaea4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 156), (3, 8), (3, 3), (3, 16), (3,)]\n"
     ]
    }
   ],
   "source": [
    "def test_load_npz(clip_length, overlapping):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "        global_to_array, n_examples = global_index_map(\n",
    "            tmpdirname, overlapping=overlapping, clip_length=clip_length\n",
    "        )\n",
    "        npz_path, indexes = global_to_array(0)\n",
    "        data = load_npz(npz_path, indexes)\n",
    "        for k in data:\n",
    "            assert data[k].shape[0] == clip_length\n",
    "        print([data[k].shape for k in data])\n",
    "\n",
    "\n",
    "test_load_npz(1, False)\n",
    "test_load_npz(3, False)\n",
    "test_load_npz(3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to iterate over all the sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpj5hliecp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb176688a965430db1e589f3d3c834a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to iterate: 1.8302652835845947\n"
     ]
    }
   ],
   "source": [
    "clip_length, overlapping = 1, False\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    global_to_array, n_examples = global_index_map(\n",
    "        tmpdirname, overlapping=overlapping, clip_length=clip_length\n",
    "    )\n",
    "    start = time.time()\n",
    "    for i in range(n_examples):\n",
    "        npz_path, indexes = global_to_array(i)\n",
    "        _ = load_npz(npz_path, indexes)\n",
    "    elapsed = time.time() - start\n",
    "print(f'time to iterate: {elapsed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp18asdg23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627353069e02474d813617e5750288b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching...\n",
      "time to cache: 3.4107885360717773\n",
      "loading from cache...\n",
      "time to iterate: 0.48926854133605957\n",
      "stored size (data+cache)/total: (2.73MB + 3.75MB)/6.49MB\n"
     ]
    }
   ],
   "source": [
    "clip_length, overlapping = 1, False\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    dir_size = lambda d: sum(file.stat().st_size for file in Path(d).rglob('*'))/1e6\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    data_size = dir_size(tmpdirname)\n",
    "    global_to_array, n_examples = global_index_map(\n",
    "        tmpdirname, overlapping=overlapping, clip_length=clip_length\n",
    "    )\n",
    "    cache_dir = os.path.join(tmpdirname, 'cache')\n",
    "    os.mkdir(cache_dir)\n",
    "    memory = joblib.Memory(cache_dir, verbose=0)\n",
    "    cached_load_npz = memory.cache(load_npz)\n",
    "    print('caching...')\n",
    "    start = time.time()\n",
    "    for i in range(n_examples):\n",
    "        npz_path, indexes = global_to_array(i)\n",
    "        _ = cached_load_npz(npz_path, indexes)\n",
    "    print(f'time to cache: {time.time() - start}')\n",
    "    print('loading from cache...')\n",
    "    start = time.time()\n",
    "    for i in range(n_examples):\n",
    "        npz_path, indexes = global_to_array(i)\n",
    "        _ = cached_load_npz(npz_path, indexes)\n",
    "    elapsed = time.time() - start\n",
    "    cache_size = dir_size(cache_dir)\n",
    "print(f'time to iterate: {elapsed}')\n",
    "print(f'stored size (data+cache)/total: ({data_size:.2f}MB + {cache_size:.2f}MB)/{cache_size+data_size:.2f}MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading from npz files and caching the result to speed everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset Class\n",
    "\n",
    "Creating a map-style PyTorch Dataset Class that uses these functions to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AMASS(Dataset):\n",
    "    def __init__(self, unpacked_directory, clip_length, overlapping,\n",
    "                 transform=None, memory=False, memory_bytes_limit=None,\n",
    "                 to_load=('poses', 'dmpls', 'trans', 'betas', 'gender')):\n",
    "        self.global_to_array, self.n_examples = global_index_map(\n",
    "            unpacked_directory, overlapping=overlapping, clip_length=clip_length\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.to_load = {}\n",
    "        for k in ('poses', 'dmpls', 'trans', 'betas', 'gender'):\n",
    "            l = f'load_{k}'\n",
    "            self.to_load[l] = True if k in to_load else False \n",
    "        caching_directory = Path(unpacked_directory) / Path('memory')\n",
    "        if memory_bytes_limit is not None:\n",
    "            warnings.warn(f'AMASS.memory.reduce_size() must be called reduce cache size to be less than {memory_bytes_limit}')\n",
    "        self.memory = joblib.Memory(caching_directory, verbose=0, bytes_limit=memory_bytes_limit)\n",
    "        self.load_npz = self.memory.cache(load_npz) if memory else load_npz\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        npz_path, array_index = self.global_to_array(i)\n",
    "        data = self.load_npz(npz_path, array_index, **self.to_load)\n",
    "        return {k: self.transform(data[k]) for k in data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test I can load some data with this Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpw_r2sl6k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e811a9a9484e8098acef3bba55ff92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([1, 156])\n",
      "dmpls torch.Size([1, 8])\n",
      "trans torch.Size([1, 3])\n",
      "betas torch.Size([1, 16])\n",
      "gender torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    amass = AMASS(tmpdirname, overlapping=False, clip_length=1, transform=torch.tensor)\n",
    "    data = amass[0]\n",
    "    for k in data:\n",
    "        print(k, data[k].shape)\n",
    "        assert type(data[k]) is torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it works in a DataLoader to make batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmptugy2bd9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cec7bf3b0d454bade89e7cdbcf738a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([4, 1, 156])\n",
      "dmpls torch.Size([4, 1, 8])\n",
      "trans torch.Size([4, 1, 3])\n",
      "betas torch.Size([4, 1, 16])\n",
      "gender torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    amass = AMASS(tmpdirname, overlapping=False, clip_length=1, transform=torch.tensor)\n",
    "    amasstrain = DataLoader(amass, batch_size=4, shuffle=True)\n",
    "    for i, data in enumerate(amasstrain):\n",
    "        if i == 0:\n",
    "            for k in data:\n",
    "                print(k, data[k].shape)\n",
    "        assert data[\"poses\"].size(0) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "Initially when I tried to load from AMASS using just a single worker (more than one worker accessing the `npz` files directly would lock up), the estimate of runtime just to iterate over the dataset was going to 160 hours. That's not practical, so I decided to use `joblib.Memory` to cache loading of the `npz` files.\n",
    "\n",
    "This also involves caching the dictionary of `viable_slices` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_amass():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Cache the data using joblib.Memory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"amassdir\",\n",
    "        type=str,\n",
    "        help=\"Directory where AMASS has been unpacked\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verify\",\n",
    "        type=\"store_true\",\n",
    "        help=\"Verify the output by calculating a checksum, \"\n",
    "        \"ensures that each tar file will only be unpacked once.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-workers\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of Dataloader workers to run the caching with\",\n",
    "    )\n",
    "    parser.add_argument(\"--bytes-limit\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"Limit of bytes to store on disk (this means some data will not be cached)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    unpack_body_models(args.tardir, args.outdir, n_jobs=args.n, verify=args.verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
